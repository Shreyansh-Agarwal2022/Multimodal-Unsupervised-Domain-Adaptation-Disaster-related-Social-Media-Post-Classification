{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5359,
     "status": "ok",
     "timestamp": 1752001181046,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "gYhnxjLcQvf-",
    "outputId": "68ffea2c-d004-4cac-f51a-0ab31397c3e6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/BXT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7601,
     "status": "ok",
     "timestamp": 1752001188644,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "ROXkfEnHQwjp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1752001188712,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "SfSqouhfQzFX",
    "outputId": "5a34a36c-9331-4f4b-b83f-c5bef0f5e2a0"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752001188718,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "3fsiDlvVQziy"
   },
   "outputs": [],
   "source": [
    "# def set_seed(seed=63):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed(seed)\n",
    "#         torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# set_seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1752001188732,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "HYsdxpSaQ1Zk"
   },
   "outputs": [],
   "source": [
    "class SourceTargetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that loads all data into GPU memory during initialization.\n",
    "    This is highly efficient for small datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_csv_path, target_csv_path, size=512):\n",
    "        # 1. Load data from CSV using pandas (remains on CPU for now)\n",
    "        feature_dtypes = {f'dim_{i}': np.float64 for i in range(size * 2)}\n",
    "        feature_dtypes['label'] = np.int64\n",
    "\n",
    "        source_df = pd.read_csv(source_csv_path, dtype=feature_dtypes)\n",
    "        target_df = pd.read_csv(target_csv_path, dtype={f'dim{i}': np.float64 for i in range(size * 2)})\n",
    "\n",
    "        feature_cols = [f'dim_{i}' for i in range(size * 2)]\n",
    "\n",
    "        # 2. Convert to NumPy arrays\n",
    "        source_features_np = source_df[feature_cols].values\n",
    "        source_labels_np = source_df['label'].values\n",
    "        target_features_np = target_df[feature_cols].values\n",
    "\n",
    "        # 3. The key change: Convert to PyTorch tensors and move to GPU *once*\n",
    "        self.source_features = torch.tensor(source_features_np, dtype=torch.float64).to(device)\n",
    "        self.source_labels = torch.tensor(source_labels_np, dtype=torch.long).to(device)\n",
    "        self.target_features = torch.tensor(target_features_np, dtype=torch.float64).to(device)\n",
    "\n",
    "        self.length = min(len(self.source_features), len(self.target_features))\n",
    "        print(f\"Loaded {self.length} samples directly to device '{device}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Data is already on the GPU, so this is just fast tensor indexing\n",
    "        return {\n",
    "            'source_features': self.source_features[idx],\n",
    "            'source_label': self.source_labels[idx],\n",
    "            'target_features': self.target_features[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "def get_data_loader(source_csv_path, target_csv_path, size=512, batch_size=32, shuffle=True, num_workers=0):\n",
    "    dataset = SourceTargetDataset(source_csv_path, target_csv_path, size=size)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,      # Must be 0 when using tensors directly from the parent process's GPU\n",
    "        pin_memory=False    # Irrelevant as data is already on the GPU\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1752001188752,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "8DiY6hAWQ3JQ"
   },
   "outputs": [],
   "source": [
    "class DomainAdaptationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that loads data from a single CSV, splits it into source and\n",
    "    target domains based on an 'event_name' column, and moves all data to the\n",
    "    specified device (e.g., GPU) during initialization.\n",
    "\n",
    "    This is highly efficient for datasets that can fit entirely in GPU memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, source_event_name, target_event_name, feature_dim=1536, label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (str): Path to the single CSV file.\n",
    "            source_event_name (str): The value in the 'event_name' column for source data.\n",
    "            target_event_name (str): The value in the 'event_name' column for target data.\n",
    "            feature_dim (int): The number of feature columns (e.g., 1536 for dim_0 to dim_1535).\n",
    "            label_col (str): The name of the column containing the labels.\n",
    "        \"\"\"\n",
    "        # 1. Define column names and dtypes for efficient loading\n",
    "        feature_cols = [f'dim_{i}' for i in range(feature_dim)]\n",
    "\n",
    "        # Define dtypes to reduce memory usage during pandas loading\n",
    "        dtypes = {col: np.float64 for col in feature_cols}\n",
    "        dtypes[label_col] = np.int64\n",
    "        dtypes['event_name'] = str\n",
    "\n",
    "        # 2. Load the entire CSV using pandas\n",
    "        print(f\"Loading data from {csv_path}...\")\n",
    "        full_df = pd.read_csv(csv_path, dtype=dtypes)\n",
    "        print(\"Data loaded into memory. Filtering by event_name...\")\n",
    "\n",
    "        # 3. Filter DataFrame to create source and target domains\n",
    "        source_df = full_df[full_df['event_name'] == source_event_name].reset_index(drop=True)\n",
    "        target_df = full_df[full_df['event_name'] == target_event_name].reset_index(drop=True)\n",
    "\n",
    "        if len(source_df) == 0:\n",
    "            raise ValueError(f\"No data found for source_event_name='{source_event_name}'\")\n",
    "        if len(target_df) == 0:\n",
    "            raise ValueError(f\"No data found for target_event_name='{target_event_name}'\")\n",
    "\n",
    "        print(f\"Found {len(source_df)} source samples and {len(target_df)} target samples.\")\n",
    "\n",
    "        # 4. Convert filtered data to NumPy arrays\n",
    "        source_features_np = source_df[feature_cols].values\n",
    "        source_labels_np = source_df[label_col].values\n",
    "        target_features_np = target_df[feature_cols].values\n",
    "        target_labels_np = target_df[label_col].values # Also get labels for target\n",
    "\n",
    "        # 5. The key step: Convert to PyTorch tensors and move to the device *once*\n",
    "        self.source_features = torch.tensor(source_features_np, dtype=torch.float64).to(device)\n",
    "        self.source_labels = torch.tensor(source_labels_np, dtype=torch.long).to(device)\n",
    "        self.target_features = torch.tensor(target_features_np, dtype=torch.float64).to(device)\n",
    "        self.target_labels = torch.tensor(target_labels_np, dtype=torch.long).to(device)\n",
    "\n",
    "        # The dataset length is the minimum of the two domains to ensure we can always pair samples\n",
    "        self.length = min(len(self.source_features), len(self.target_features))\n",
    "        print(f\"Dataset initialized. Length set to {self.length} (minimum of source/target). Data is on device '{device}'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Data is already on the device, so this is just fast tensor indexing.\n",
    "        # Note: If shuffle=True in DataLoader, the indices 'idx' will be shuffled.\n",
    "        # This implementation pairs the i-th source sample with the i-th target sample\n",
    "        # from their respective (shuffled) lists.\n",
    "        return {\n",
    "            'source_features': self.source_features[idx],\n",
    "            'source_label':    self.source_labels[idx],\n",
    "            'target_features': self.target_features[idx],\n",
    "            'target_label':    self.target_labels[idx] # Return target label for evaluation\n",
    "        }\n",
    "\n",
    "def get_domain_adaptation_loader(csv_path, source_event_name, target_event_name, feature_dim=1536, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates and returns a DataLoader for domain adaptation.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the single CSV file.\n",
    "        source_event_name (str): The event name for the source domain.\n",
    "        target_event_name (str): The event name for the target domain.\n",
    "        feature_dim (int): The number of feature dimensions.\n",
    "        batch_size (int): The size of each batch.\n",
    "        shuffle (bool): Whether to shuffle the data at each epoch.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch DataLoader instance.\n",
    "    \"\"\"\n",
    "    dataset = DomainAdaptationDataset(\n",
    "        csv_path=csv_path,\n",
    "        source_event_name=source_event_name,\n",
    "        target_event_name=target_event_name,\n",
    "        feature_dim=feature_dim\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,      # MUST be 0 when data is on CUDA in the main process\n",
    "        pin_memory=False    # Irrelevant as data is already on the target device\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1752001188775,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "O4OGO5AqQ5z2"
   },
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.lambda_, None\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 768),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# BlueSky + CrisisMMD\n",
    "# class FeatureExtractor(nn.Module):\n",
    "#     def __init__(self, input_size, num_heads=8):\n",
    "#         \"\"\"\n",
    "#         input_size: total feature dim (e.g. 1536)\n",
    "#         Assumes two tokens of size input_size//2 each.\n",
    "#         \"\"\"\n",
    "#         super(FeatureExtractor, self).__init__()\n",
    "#         assert input_size % 2 == 0, \"input_size must be divisible by 2\"\n",
    "#         self.token_dim = input_size // 2\n",
    "\n",
    "#         # Cross‑attention on the two tokens\n",
    "#         self.attention = nn.MultiheadAttention(\n",
    "#             embed_dim=self.token_dim,\n",
    "#             num_heads=num_heads,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_size, 768),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(768, 512),\n",
    "#             nn.LeakyReLU(0.2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         token1 = x[:, :self.token_dim]\n",
    "#         token2 = x[:, self.token_dim:]\n",
    "#         tokens = torch.stack([token1, token2], dim=1)\n",
    "\n",
    "#         attn_output, _ = self.attention(tokens, tokens, tokens)\n",
    "#         attended = attn_output.reshape(x.size(0), -1)\n",
    "\n",
    "#         return self.model(attended)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            GradientReversalLayer(),\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1752001188779,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "L7J1QVo3RAID"
   },
   "outputs": [],
   "source": [
    "def plot_tsne(feature_extractor, dataset, device, num_points=2000, title=\"t-SNE Visualization\"):\n",
    "    \"\"\"\n",
    "    Generates and displays a t-SNE plot for features from source and target domains.\n",
    "\n",
    "    Args:\n",
    "        feature_extractor (nn.Module): The trained feature extractor model (G).\n",
    "        dataset (SourceTargetDataset): The dataset object containing source and target data.\n",
    "        device (torch.device): The device the data and models are on.\n",
    "        num_points (int): The number of points to sample from EACH domain (total points = 2 * num_points).\n",
    "        title (str): The title for the plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating t-SNE plot with {num_points} points from each domain (total {num_points*2})...\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    # Ensure we don't sample more points than available\n",
    "    num_source_samples = min(num_points, len(dataset.source_features))\n",
    "    num_target_samples = min(num_points, len(dataset.target_features))\n",
    "\n",
    "    # Generate random indices for sampling\n",
    "    source_indices = torch.randperm(len(dataset.source_features), device=device)[:num_source_samples]\n",
    "    target_indices = torch.randperm(len(dataset.target_features), device=device)[:num_target_samples]\n",
    "\n",
    "    # Sample data from the dataset (which is already on the GPU)\n",
    "    source_data_sample = dataset.source_features[source_indices]\n",
    "    target_data_sample = dataset.target_features[target_indices]\n",
    "\n",
    "    # Generate embeddings using the feature extractor\n",
    "    with torch.no_grad():\n",
    "        source_embeddings = feature_extractor(source_data_sample)\n",
    "        target_embeddings = feature_extractor(target_data_sample)\n",
    "\n",
    "    # Move embeddings to CPU for scikit-learn\n",
    "    source_cpu = source_embeddings.cpu().numpy()\n",
    "    target_cpu = target_embeddings.cpu().numpy()\n",
    "\n",
    "    # Combine embeddings from both domains\n",
    "    combined_embeddings = np.vstack((source_cpu, target_cpu))\n",
    "\n",
    "    # Create labels to distinguish domains (0 for source, 1 for target)\n",
    "    source_domain_labels = np.zeros(num_source_samples)\n",
    "    target_domain_labels = np.ones(num_target_samples)\n",
    "    combined_labels = np.concatenate((source_domain_labels, target_domain_labels))\n",
    "\n",
    "    # Perform t-SNE\n",
    "    print(\"Running t-SNE... This may take a moment.\")\n",
    "    tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(combined_embeddings)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=combined_labels, cmap='coolwarm', alpha=0.7, s=15)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
    "    plt.ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
    "\n",
    "    # Create a clear legend\n",
    "    handles, _ = scatter.legend_elements()\n",
    "    plt.legend(handles, ['Source Domain', 'Target Domain'], title=\"Domain\", fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    # Set the model back to train mode for any further use\n",
    "    feature_extractor.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1752001188839,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "GSFTxRzZRAoj"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title):\n",
    "    \"\"\"Helper function to create a visual confusion matrix using seaborn.\"\"\"\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d', # format as integer\n",
    "        cmap='Blues',\n",
    "        xticklabels=['Predicted 0 (Neg)', 'Predicted 1 (Pos)'],\n",
    "        yticklabels=['Actual 0 (Neg)', 'Actual 1 (Pos)']\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_scores, title='ROC Curve'):\n",
    "    \"\"\"\n",
    "    Calculates and plots the ROC curve and prints the AUC score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True binary labels.\n",
    "        y_scores (np.array): Target scores, can either be probability estimates of the\n",
    "                             positive class or confidence values.\n",
    "        title (str): The title for the plot.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated Area Under the Curve (AUC).\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc # Return the AUC score for printing\n",
    "\n",
    "def test_model_on_new_file(G, C, test_csv_path, source_event_name, target_event_name, feature_dim=1536, label_col='label'):\n",
    "    \"\"\"\n",
    "    Loads data from a new test CSV, evaluates the trained models (G and C) on it,\n",
    "    and prints detailed metrics, confusion matrices, and ROC curves.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Final Evaluation on Test File ---\")\n",
    "    print(f\"Loading test data from: {test_csv_path}\")\n",
    "\n",
    "    # 1. Load and process the new CSV data\n",
    "    feature_cols = [f'dim_{i}' for i in range(feature_dim)]\n",
    "    dtypes = {col: np.float64 for col in feature_cols}\n",
    "    dtypes[label_col] = np.int64\n",
    "    dtypes['event_name'] = str\n",
    "    test_df = pd.read_csv(test_csv_path, dtype=dtypes)\n",
    "\n",
    "    source_df = test_df[test_df['event_name'] == source_event_name]\n",
    "    target_df = test_df[test_df['event_name'] == target_event_name]\n",
    "\n",
    "    if len(source_df) == 0 or len(target_df) == 0:\n",
    "        print(\"Warning: No data found for one or both event names in the test file.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(source_df)} source and {len(target_df)} target samples in the test file.\")\n",
    "\n",
    "    # Convert to Tensors and move to device\n",
    "    source_features = torch.tensor(source_df[feature_cols].values, dtype=torch.float64).to(device)\n",
    "    source_labels = torch.tensor(source_df[label_col].values, dtype=torch.long).to(device)\n",
    "    target_features = torch.tensor(target_df[feature_cols].values, dtype=torch.float64).to(device)\n",
    "    target_labels = torch.tensor(target_df[label_col].values, dtype=torch.long).to(device)\n",
    "\n",
    "    # 2. Perform inference\n",
    "    G.eval()\n",
    "    C.eval()\n",
    "    with torch.no_grad():\n",
    "        # Source domain\n",
    "        source_feats_extracted = G(source_features)\n",
    "        source_outputs = C(source_feats_extracted)\n",
    "        source_probs = F.softmax(source_outputs, dim=1)[:, 1] # Get probs for class 1\n",
    "        _, source_predicted = torch.max(source_outputs.data, 1)\n",
    "\n",
    "        # Target domain\n",
    "        target_feats_extracted = G(target_features)\n",
    "        target_outputs = C(target_feats_extracted)\n",
    "        target_probs = F.softmax(target_outputs, dim=1)[:, 1] # Get probs for class 1\n",
    "        _, target_predicted = torch.max(target_outputs.data, 1)\n",
    "\n",
    "        # Move all results to CPU for sklearn\n",
    "        all_source_preds = source_predicted.cpu().numpy()\n",
    "        all_source_labels = source_labels.cpu().numpy()\n",
    "        all_source_scores = source_probs.cpu().numpy()\n",
    "\n",
    "        all_target_preds = target_predicted.cpu().numpy()\n",
    "        all_target_labels = target_labels.cpu().numpy()\n",
    "        all_target_scores = target_probs.cpu().numpy()\n",
    "\n",
    "    # 3. Calculate and display metrics\n",
    "    def print_final_metrics(domain_name, y_true, y_pred, y_scores):\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "        print(f\"\\n--- Final Test Metrics: {domain_name} Domain ---\")\n",
    "        print(f\"Accuracy:                {accuracy:.4f}\")\n",
    "        print(f\"Precision (for class 1): {precision:.4f}\")\n",
    "        print(f\"Recall (for class 1):    {recall:.4f}\")\n",
    "        print(f\"F1-Score (for class 1):  {f1:.4f}\")\n",
    "\n",
    "        # Plot ROC curve and get AUC\n",
    "        roc_auc = plot_roc_curve(y_true, y_scores, title=f'Final Test - {domain_name} ROC Curve')\n",
    "        print(f\"AUC-ROC:                 {roc_auc:.4f}\")\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(cm, title=f'Final Test - {domain_name} Confusion Matrix')\n",
    "\n",
    "    print_final_metrics(\"Source\", all_source_labels, all_source_preds, all_source_scores)\n",
    "    print_final_metrics(\"Target\", all_target_labels, all_target_preds, all_target_scores)\n",
    "    print(\"\\n--- Final Evaluation Complete ---\")\n",
    "\n",
    "\n",
    "def evaluate_domain_performance(G, C, test_csv_path, domain_name, feature_dim=1536, label_col='label'):\n",
    "    \"\"\"\n",
    "    Loads data from a test CSV and evaluates the models, ensuring data is float64.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Evaluating: {domain_name} Domain ---\")\n",
    "    print(f\"Loading data from: {test_csv_path}\")\n",
    "\n",
    "    # 1. Load and process the data\n",
    "    feature_cols = [f'dim_{i}' for i in range(feature_dim)]\n",
    "    try:\n",
    "        test_df = pd.read_csv(test_csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {test_csv_path}\")\n",
    "        return\n",
    "\n",
    "    # CORRECTED LINE: Ensure the dtype is torch.float64 to match your model's weights.\n",
    "    features = torch.tensor(test_df[feature_cols].values, dtype=torch.float64).to(device)\n",
    "    # Alternatively, you could just let it use the default you set:\n",
    "    # features = torch.tensor(test_df[feature_cols].values).to(device)\n",
    "\n",
    "    labels = torch.tensor(test_df[label_col].values, dtype=torch.long).to(device)\n",
    "\n",
    "    # 2. Perform inference\n",
    "    G.eval()\n",
    "    C.eval()\n",
    "    with torch.no_grad():\n",
    "        # This call will now work with your float64 model\n",
    "        extracted_features = G(features)\n",
    "        outputs = C(extracted_features)\n",
    "        probs = F.softmax(outputs, dim=1)[:, 1]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Move results to CPU for sklearn\n",
    "        all_preds = predicted.cpu().numpy()\n",
    "        all_labels = labels.cpu().numpy()\n",
    "        all_scores = probs.cpu().numpy()\n",
    "\n",
    "    # 3. Calculate and display metrics (This part remains the same)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])\n",
    "    roc_auc = roc_auc_score(all_labels, all_scores)\n",
    "\n",
    "    print(f\"\\n** Test Metrics for {domain_name} Domain **\")\n",
    "    print(f\"Accuracy:                {accuracy:.4f}\")\n",
    "    print(f\"Precision (for class 1): {precision:.4f}\")\n",
    "    print(f\"Recall (for class 1):    {recall:.4f}\")\n",
    "    print(f\"F1-Score (for class 1):  {f1:.4f}\")\n",
    "    print(f\"AUC-ROC:                 {roc_auc:.4f}\")\n",
    "\n",
    "    # 4. Plotting\n",
    "    plot_roc_curve(all_labels, all_scores, title=f'Test - {domain_name} ROC Curve')\n",
    "    plot_confusion_matrix(cm, title=f'Test - {domain_name} Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1752001188934,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "JXY7AnwoRU36"
   },
   "outputs": [],
   "source": [
    "class DomainAdaptation():\n",
    "\n",
    "    def __init__(self, source_path, target_path, num_classes, size = 512, batch_size=16, epochs = 20, csv_path = None):\n",
    "\n",
    "\n",
    "        # self.dataloader = get_domain_adaptation_loader(\n",
    "        #     csv_path=csv_path,\n",
    "        #     source_event_name=source_path,\n",
    "        #     target_event_name=target_path,\n",
    "        #     feature_dim=size*2,\n",
    "        #     batch_size=batch_size\n",
    "        # )\n",
    "\n",
    "        self.dataloader = get_data_loader(source_path, target_path, size = size, batch_size=batch_size)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.G = FeatureExtractor(input_size=size*2).to(device)               # Generator\n",
    "        self.C = Classifier(input_size=512, num_classes=self.num_classes).to(device)   # Classifier\n",
    "        self.C1 = Classifier(input_size=512, num_classes=self.num_classes).to(device)  # Classifier-1\n",
    "        self.C2 = Classifier(input_size=512, num_classes=self.num_classes).to(device)  # Classifier-2\n",
    "        self.D = Discriminator(input_size=512).to(device)                   # Discriminator\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = 5e-4\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.set_optimizer()                # Setting Up Adam Optimizer\n",
    "        self.reset_grad()\n",
    "\n",
    "        # Saving Pseudo-Labels of Target Domain Batch\n",
    "        self.output_cr_t_C_label = np.zeros(self.batch_size)\n",
    "\n",
    "\n",
    "    def set_optimizer(self):\n",
    "        self.opt_g = optim.Adam(self.G.parameters(), lr=self.lr)\n",
    "        self.opt_c = optim.Adam(self.C.parameters(), lr=self.lr)\n",
    "        self.opt_c1 = optim.Adam(self.C1.parameters(), lr=self.lr)\n",
    "        self.opt_c2 = optim.Adam(self.C2.parameters(), lr=self.lr)\n",
    "        self.opt_d = optim.Adam(self.D.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.opt_g.zero_grad()\n",
    "        self.opt_c.zero_grad()\n",
    "        self.opt_c1.zero_grad()\n",
    "        self.opt_c2.zero_grad()\n",
    "        self.opt_d.zero_grad()\n",
    "\n",
    "\n",
    "    def discrepancy(self, out1, out2):\n",
    "        return torch.mean(torch.abs(F.softmax(out1, dim=1) - F.softmax(out2, dim=1)))\n",
    "\n",
    "\n",
    "    def linear_mmd(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X - f_of_Y\n",
    "        loss = torch.mean(torch.mm(delta, torch.transpose(delta, 0, 1)))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def ent(self, output):\n",
    "        return -torch.mean(F.softmax(output, dim=1) * F.log_softmax(output, dim=1))\n",
    "\n",
    "    def test_target_domain(self, epoch):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the target domain using the main classifier C.\n",
    "        This is a quick check and doesn't show all metrics.\n",
    "        \"\"\"\n",
    "        self.G.eval()\n",
    "        self.C.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in self.dataloader:\n",
    "                target_features = batch['target_features'].to(device)\n",
    "                target_labels = batch['target_label'].to(device)\n",
    "                features = self.G(target_features)\n",
    "                outputs = self.C(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += target_labels.size(0)\n",
    "                total_correct += (predicted == target_labels).sum().item()\n",
    "        accuracy = 100 * total_correct / total_samples\n",
    "        print(f\"\\n--- Quick Epoch {epoch} Target Accuracy ---\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}% ({total_correct}/{total_samples})\")\n",
    "        print(\"---------------------------------------\\n\")\n",
    "        self.G.train()\n",
    "        self.C.train()\n",
    "\n",
    "    def evaluate_and_print_metrics(self, epoch):\n",
    "        \"\"\"\n",
    "        Performs a full evaluation on the entire source and target sets during training.\n",
    "        \"\"\"\n",
    "        self.G.eval()\n",
    "        self.C.eval()\n",
    "\n",
    "        source_features = self.dataloader.dataset.source_features\n",
    "        source_labels = self.dataloader.dataset.source_labels\n",
    "        target_features = self.dataloader.dataset.target_features\n",
    "        target_labels = self.dataloader.dataset.target_labels\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Source domain inference\n",
    "            source_feats_extracted = self.G(source_features)\n",
    "            source_outputs = self.C(source_feats_extracted)\n",
    "            source_probs = F.softmax(source_outputs, dim=1)[:, 1]\n",
    "            _, source_predicted = torch.max(source_outputs.data, 1)\n",
    "\n",
    "            # Target domain inference\n",
    "            target_feats_extracted = self.G(target_features)\n",
    "            target_outputs = self.C(target_feats_extracted)\n",
    "            target_probs = F.softmax(target_outputs, dim=1)[:, 1]\n",
    "            _, target_predicted = torch.max(target_outputs.data, 1)\n",
    "\n",
    "            # Move results to CPU for sklearn\n",
    "            all_source_preds = source_predicted.cpu().numpy()\n",
    "            all_source_labels = source_labels.cpu().numpy()\n",
    "            all_source_scores = source_probs.cpu().numpy()\n",
    "\n",
    "            all_target_preds = target_predicted.cpu().numpy()\n",
    "            all_target_labels = target_labels.cpu().numpy()\n",
    "            all_target_scores = target_probs.cpu().numpy()\n",
    "\n",
    "        print(f\"\\n--- Full Epoch {epoch} Evaluation ---\")\n",
    "\n",
    "        def print_metrics(domain_name, y_true, y_pred, y_scores):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "\n",
    "            print(f\"--- {domain_name} Domain Metrics ---\")\n",
    "            print(f\"Accuracy:                {accuracy:.4f}\")\n",
    "            print(f\"Precision (for class 1): {precision:.4f}\")\n",
    "            print(f\"Recall (for class 1):    {recall:.4f}\")\n",
    "            print(f\"F1-Score (for class 1):  {f1:.4f}\")\n",
    "\n",
    "            # Plot ROC and get AUC\n",
    "            roc_auc = plot_roc_curve(y_true, y_scores, title=f'Epoch {epoch} - {domain_name} ROC Curve')\n",
    "            print(f\"AUC-ROC:                 {roc_auc:.4f}\")\n",
    "\n",
    "            # Plot confusion matrix\n",
    "            plot_confusion_matrix(cm, title=f'Epoch {epoch} - {domain_name} Confusion Matrix')\n",
    "\n",
    "\n",
    "        print_metrics(\"Source\", all_source_labels, all_source_preds, all_source_scores)\n",
    "        print_metrics(\"Target\", all_target_labels, all_target_preds, all_target_scores)\n",
    "        print(\"--------------------------------\\n\")\n",
    "\n",
    "        self.G.train()\n",
    "        self.C.train()\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        A_st_min = 0.0\n",
    "        A_st_max = 0.0\n",
    "        min_J_w = 1.0\n",
    "        max_J_w = 1.0\n",
    "        A_st_norm = 0.5\n",
    "        J_w_norm = 0.5\n",
    "\n",
    "        # Switched to CrossEntropyLoss for multi-class classification\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        discriminator_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.G.train()\n",
    "            self.C.train()\n",
    "            self.C1.train()\n",
    "            self.C2.train()\n",
    "            self.D.train()\n",
    "            fea_for_LDA = np.empty(shape=(0, 512))\n",
    "            fea_s_for_LDA = np.empty(shape=(0, 512))\n",
    "            label_for_LDA = np.empty(shape=(0, 1))\n",
    "            label_s_for_LDA = []\n",
    "\n",
    "            # Training loop\n",
    "            for batch_idx, batch in enumerate(tqdm(self.dataloader, desc=f\"Epoch {epoch+1}/{self.epochs}\")):\n",
    "                source_features = batch['source_features']\n",
    "                # Ensure labels are of type Long for CrossEntropyLoss\n",
    "                source_labels = batch['source_label']\n",
    "                target_features = batch['target_features']\n",
    "\n",
    "                actual_batch_size = source_features.size(0)\n",
    "                if actual_batch_size != self.batch_size:\n",
    "                    continue\n",
    "\n",
    "                self.reset_grad()\n",
    "\n",
    "                # T_complex = A_st_norm /(A_st_norm + (1.0 - J_w_norm))\n",
    "                # T = T_complex.real\n",
    "                T = 0.3\n",
    "\n",
    "                # Normal Supervised Learning on Source Domain Batch\n",
    "                for i in range(1):\n",
    "                    feat_cr_s = self.G(source_features)\n",
    "                    output_cr_s_C = self.C(feat_cr_s)\n",
    "                    loss_1 = criterion(output_cr_s_C, source_labels)\n",
    "\n",
    "                    loss_1.backward()\n",
    "                    self.opt_g.step()\n",
    "                    self.opt_c.step()\n",
    "                    self.reset_grad()\n",
    "\n",
    "\n",
    "                # Discriminator Training\n",
    "                # Normal Supervised Learning of C1 and C2\n",
    "\t\t\t\t# Maximize Discripency between C1 and C2\n",
    "                for i in range(1):\n",
    "\n",
    "                    feat_cr_s = self.G(source_features)\n",
    "                    feat_cr_t = self.G(target_features)\n",
    "                    output_cr_s_D = self.D(feat_cr_s)\n",
    "                    output_cr_t_D = self.D(feat_cr_t)\n",
    "\n",
    "                    domain_labels_s = torch.ones(actual_batch_size, dtype=torch.long, device=device)\n",
    "                    domain_labels_t = torch.zeros(actual_batch_size, dtype=torch.long, device=device)\n",
    "\n",
    "                    loss_4 = discriminator_criterion(output_cr_s_D, domain_labels_s) + \\\n",
    "\t\t\t\t\t\t\tdiscriminator_criterion(output_cr_t_D, domain_labels_t)\n",
    "\n",
    "                    loss_4 = 0.1 * loss_4\n",
    "\n",
    "                    loss_4.backward()\n",
    "                    self.opt_d.step()\n",
    "                    self.reset_grad()\n",
    "\n",
    "                    feat_cr_t = self.G(target_features)\n",
    "                    feat_cr_s = self.G(source_features)\n",
    "\n",
    "                    output_cr_s_C1 = self.C1(feat_cr_s)\n",
    "                    output_cr_s_C2 = self.C2(feat_cr_s)\n",
    "\n",
    "                    output_cr_t_C1 = self.C1(feat_cr_t)\n",
    "                    output_cr_t_C2 = self.C2(feat_cr_t)\n",
    "\n",
    "                    loss_dis1_t = -self.discrepancy(output_cr_t_C1, output_cr_t_C2) + criterion(output_cr_s_C1, source_labels) + criterion(output_cr_s_C2, source_labels)\n",
    "                    loss_dis1_t.backward()\n",
    "\n",
    "                    self.opt_c1.step()\n",
    "                    self.opt_c2.step()\n",
    "                    self.reset_grad()\n",
    "\n",
    "\n",
    "                # Balance of transferability and discriminability\n",
    "                for i in range(4):\n",
    "                    feat_cr_s = self.G(source_features)\n",
    "                    feat_cr_t = self.G(target_features)\n",
    "                    output_cr_s_D = self.D(feat_cr_s)\n",
    "                    output_cr_t_D = self.D(feat_cr_t)\n",
    "\n",
    "                    domain_labels_s = torch.ones(actual_batch_size, dtype=torch.long, device=device)\n",
    "                    domain_labels_t = torch.zeros(actual_batch_size, dtype=torch.long, device=device)\n",
    "\n",
    "                    loss_4 = discriminator_criterion(output_cr_s_D, domain_labels_s) + \\\n",
    "\t\t\t\t\t\t\tdiscriminator_criterion(output_cr_t_D, domain_labels_t)\n",
    "\n",
    "                    loss_4 = 0.4 * loss_4\n",
    "\n",
    "                    output_cr_t_C = self.C(feat_cr_t)\n",
    "                    output_cr_t_C1 = self.C1(feat_cr_t)\n",
    "                    output_cr_t_C2 = self.C2(feat_cr_t)\n",
    "\n",
    "                    loss_51 = self.discrepancy(output_cr_t_C1, output_cr_t_C2)\n",
    "                    loss_52 = self.discrepancy(output_cr_t_C, output_cr_t_C1)\n",
    "                    loss_53 = self.discrepancy(output_cr_t_C, output_cr_t_C2)\n",
    "                    loss_5 = loss_52 + loss_53 + loss_51\n",
    "\n",
    "                    loss_all = (T*loss_4 + (1.0-T)*loss_5)\n",
    "                    loss_all.backward()\n",
    "                    self.opt_g.step()\n",
    "                    self.reset_grad()\n",
    "\n",
    "                # Only use for DANN\n",
    "                # for i in range(1):\n",
    "                #     feat_cr_s = self.G(source_features)\n",
    "                #     feat_cr_t = self.G(target_features)\n",
    "                #     output_cr_s_D = self.D(feat_cr_s)\n",
    "                #     output_cr_t_D = self.D(feat_cr_t)\n",
    "\n",
    "                #     domain_labels_s = torch.ones(actual_batch_size, dtype=torch.long, device=device)\n",
    "                #     domain_labels_t = torch.zeros(actual_batch_size, dtype=torch.long, device=device)\n",
    "\n",
    "                #     loss_4 = discriminator_criterion(output_cr_s_D, domain_labels_s) + \\\n",
    "\t\t\t\t# \t\t\tdiscriminator_criterion(output_cr_t_D, domain_labels_t)\n",
    "\n",
    "                #     loss_4 = 0.4 * loss_4\n",
    "\n",
    "                #     loss_all = loss_4\n",
    "                #     loss_all.backward()\n",
    "                #     self.opt_g.step()\n",
    "                #     self.opt_d.step()\n",
    "                #     self.reset_grad()\n",
    "\n",
    "                # with torch.no_grad():\n",
    "                #     feat_cr_s = self.G(source_features)\n",
    "                #     feat_cr_t = self.G(target_features)\n",
    "                #     label_predi = self.C(feat_cr_t)\n",
    "\n",
    "                #     feat_s_test_np = feat_cr_s.cpu().detach().numpy()\n",
    "                #     label_s_test_np = source_labels.cpu().detach().numpy()\n",
    "\n",
    "                #     label_s_for_LDA = np.append(label_s_for_LDA, label_s_test_np.flatten())\n",
    "                #     fea_s_for_LDA = np.vstack((fea_s_for_LDA, feat_s_test_np)) if fea_s_for_LDA.size > 0 else feat_s_test_np\n",
    "\n",
    "                #     feat_test_np = feat_cr_t.cpu().detach().numpy()\n",
    "                #     fea_for_LDA = np.vstack((fea_for_LDA, feat_test_np)) if fea_for_LDA.size > 0 else feat_test_np\n",
    "\n",
    "                #     # Convert pseudo-labels for multi-class\n",
    "                #     label_t = torch.argmax(label_predi, dim=1)\n",
    "                #     label_test_np = label_t.cpu().detach().numpy().reshape(-1, 1)\n",
    "                #     label_for_LDA = np.vstack((label_for_LDA, label_test_np)) if label_for_LDA.size > 0 else label_test_np\n",
    "\n",
    "            if (epoch + 1) % 5 == 1 or epoch + 1 == self.epochs:\n",
    "                # self.evaluate_and_print_metrics(epoch + 1)\n",
    "                evaluate_domain_performance(self.G, self.C, \"test0_clip.csv\", \"Source\")\n",
    "                evaluate_domain_performance(self.G, self.C, \"bluesky_test0.csv\", \"Target\")\n",
    "\n",
    "            if (epoch + 1) % 5 == 1 or epoch + 1 == self.epochs:\n",
    "                plot_tsne(\n",
    "                    feature_extractor=self.G,\n",
    "                    dataset=self.dataloader.dataset, # Get the dataset from the dataloader\n",
    "                    device=device,\n",
    "                    num_points=2000, # Samples 2000 from source and 2000 from target\n",
    "                    title=\"t-SNE of Source and Target Features After Training\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 381116,
     "status": "ok",
     "timestamp": 1752001795404,
     "user": {
      "displayName": "Shreyansh Agarwal",
      "userId": "03440258780000154060"
     },
     "user_tz": -330
    },
    "id": "9zs_MO61RZEm",
    "outputId": "3d302c3f-f572-4d0f-9481-df37f957738f"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "     num_classes = 2\n",
    "    #  source_event_name = \"hurricane_harvey\"\n",
    "    #  target_event_name = \"hurricane_irma\"\n",
    "\n",
    "     source_path = \"train_clip0.csv\"\n",
    "     target_path = \"bluesky_clip.csv\"\n",
    "\n",
    "     size = 768\n",
    "     model = DomainAdaptation(source_path, target_path, num_classes=num_classes, size=size, batch_size=128, epochs=50)\n",
    "     #model = DomainAdaptation(source_event_name, target_event_name , num_classes=num_classes, size=size, batch_size=128, epochs=100, csv_path=\"train_clip0.csv\")\n",
    "     model.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
